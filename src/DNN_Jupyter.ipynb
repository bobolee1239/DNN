{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### !/usr/bin/env python3\n",
    "#\n",
    "## ---------- Deep Learning Hw1 -----------\n",
    "# ID: A061508\n",
    "#\n",
    "## ----------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import ActiveFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveFunctions:\n",
    "    \"\"\"\n",
    "    A base class of activation functions.\n",
    "    \"\"\"\n",
    "    def at(self, rawValues):\n",
    "        return rawValues\n",
    "    \n",
    "    def gradientAt(self, rawValues):\n",
    "        return np.array([1.0])\n",
    "\n",
    "class ReLu(ActiveFunctions):\n",
    "    \"\"\"\n",
    "    ReLu activation function.\n",
    "    \"\"\"\n",
    "    def at(self, rawValues):\n",
    "        if type(rawValues) != np.ndarray:\n",
    "            return rawValues if rawValues > 0.0 else 0.0\n",
    "        \n",
    "        output = np.zeros(rawValues.shape)\n",
    "        for idx, value in enumerate(rawValues):\n",
    "            output[idx] = value if value > 0.0 else 0.0\n",
    "        return output\n",
    "    \n",
    "    def gradientAt(self, rawValues):\n",
    "        if type(rawValues) != np.ndarray:\n",
    "            return 1.0 if rawValues > 0.0 else 0.0\n",
    "        \n",
    "        output = np.zeros(rawValues.shape)\n",
    "        for idx, value in enumerate(rawValues):\n",
    "            output[idx] = 1.0 if value > 0.0 else 0.0\n",
    "        return output\n",
    "    \n",
    "class Softmax(ActiveFunctions):\n",
    "    \"\"\"\n",
    "    Softmax activation function.\n",
    "    \"\"\"\n",
    "    def at(self, rawValues):\n",
    "        if type(rawValues) is not np.ndarray: \n",
    "            raise TypeError('<Usage> Softmax.at(vars) where type vars should be numpy.ndarrya!')\n",
    "        \n",
    "        rawValues = rawValues.ravel()\n",
    "        maxValue = max(rawValues)\n",
    "        rawValues -= maxValue\n",
    "        output = np.zeros(rawValues.shape)\n",
    "        for idx, value in enumerate(rawValues):\n",
    "            output[idx] = np.exp(value) / sum(np.exp(rawValues))\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def gradientAt(self, rawValues):\n",
    "        if type(rawValues) is not np.ndarray: \n",
    "            raise TypeError('<Usage> Softmax.at(vars) where type vars should be numpy.ndarrya!')\n",
    "            \n",
    "        rawValues = rawValues.ravel()\n",
    "        maxValue = max(rawValues)\n",
    "        rawValues -= maxValue    \n",
    "        \n",
    "        output = np.zeros(rawValues.shape)\n",
    "        for idx, value in enumerate(rawValues):\n",
    "            p = np.exp(value) / sum(np.exp(rawValues))\n",
    "            output[idx] = p * (1 - p)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fully Connected LAYER\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    A base class to represent a layer as a list of node and a table of output\n",
    "    weights.\n",
    "    \"\"\"\n",
    "    stdev = 0.01\n",
    "\n",
    "    def __init__(self, numberOfInput, numberOfNodes, activeFunc = ActiveFunctions()):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "        ------------------------------------------------------------\n",
    "        numberOfInputNodes <Int> : number of nodes of last layer.\n",
    "        numberOfNodes <Int> : number of nodes this layer.\n",
    "        activeFunc <Function Pointer> : activer function, e.g. softmax, reLu ...\n",
    "        \"\"\"\n",
    "        self.numberOfInput = numberOfInput\n",
    "        self.numberOfNodes = numberOfNodes\n",
    "        self.activeFunc = activeFunc\n",
    "        self.rawOutputs = []\n",
    "        self.outputs = []\n",
    "        \n",
    "        \n",
    "        # randomly initialize parameters\n",
    "        self.weightTable = np.random.normal(0, Layer.stdev,   \\\n",
    "                                              (numberOfNodes, numberOfInput))\n",
    "        self.biases = np.random.normal(0, Layer.stdev, (numberOfNodes, ))\n",
    "        \n",
    "        # Reserve memory for gradients\n",
    "        self.gradientOfBiases = np.zeros(self.biases.shape)\n",
    "        self.gradientOfWeightTable = np.zeros(self.weightTable.shape)\n",
    "\n",
    "    def computeOutput(self, inputData, toUpdate=False):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "        ------------------------------------------------------------\n",
    "        input: <np.ndarray> a couple of input values in the shape of (self.numberOfInputNodes, ).\n",
    "\n",
    "        Return:\n",
    "        ------------------------------------------------------------\n",
    "        output: <np.ndarray> output values in the shape of (self.numberOfNodes, ).\n",
    "        \"\"\"\n",
    "        # Reserve output memories\n",
    "        rawOutput = self.weightTable.dot(inputData.reshape(self.numberOfInput, 1)) + self.biases.reshape(self.numberOfNodes, 1)\n",
    "        output = self.activeFunc.at(rawOutput.ravel())\n",
    "        \n",
    "        # Store RawOutput ans Output\n",
    "        if toUpdate:\n",
    "            self.rawOutputs.append(rawOutput.ravel())\n",
    "            self.outputs.append(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def computeOutputs(self, inputDatas, toUpdate = False):\n",
    "        results = []\n",
    "        for data in inputDatas:\n",
    "            results.append(self.computeOutput(data, toUpdate))\n",
    "        return results\n",
    "    \n",
    "    def update(self, preGradients, lastLayerOutput, lr, toPrint = False):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "        --------------------------\n",
    "        - preGradients <list of np.ndarray> : store multiple gradients from next layer.\n",
    "        \n",
    "        \"\"\"\n",
    "        assert type(preGradients[0]) == np.ndarray, '<Usage> AnLayer.update(preGradients) where preGradients should be an numpy.ndarray'\n",
    "        \n",
    "        gradients = []      # store gradients to propagation\n",
    "        gradientsOfBiases = []\n",
    "        gradientsOfWeightTable = []\n",
    "        \n",
    "        for idx, preGradient in enumerate(preGradients):\n",
    "            preGradient = preGradient.reshape(len(preGradient), 1)\n",
    "            tmpGradient = preGradient * self.activeFunc.gradientAt(self.rawOutputs[idx]).reshape(len(preGradient), 1)\n",
    "            tmpGradientOfBiases = tmpGradient #+ 0.01*(self.biases.reshape(len(tmpGradient), 1)**2)\n",
    "            \n",
    "            if toPrint:\n",
    "                print('tmpGradient:', tmpGradient)\n",
    "                print('preGradient:', preGradient)\n",
    "                print('activeFunc:', self.activeFunc.gradientAt(self.rawOutputs[idx]))\n",
    "                print('index:', idx)\n",
    "                print('----------------')\n",
    "                \n",
    "            tmpGradientOfWeightTable = tmpGradient.reshape(len(tmpGradient), 1).dot(\n",
    "                lastLayerOutput[idx].reshape(1, len(lastLayerOutput[idx]))) #+ 0.01*(self.weightTable**2)\n",
    "            \n",
    "            gradient = self.weightTable.T.dot(tmpGradient.reshape(len(tmpGradient), 1))\n",
    "            \n",
    "            # store values\n",
    "            gradients.append(gradient)\n",
    "            gradientsOfBiases.append(tmpGradientOfBiases)\n",
    "            gradientsOfWeightTable.append(tmpGradientOfWeightTable)\n",
    "            \n",
    "        # compute stochastic gradient\n",
    "        sgOfBiases = sum(gradientsOfBiases) / float(len(gradientsOfBiases))\n",
    "        sgOfWeightTable = sum(gradientsOfWeightTable) / float(len(gradientsOfWeightTable))\n",
    "        \n",
    "        if toPrint:\n",
    "            print('sgOfBiases:', lr * sgOfBiases)\n",
    "            print('sgOfWeightTable', lr * sgOfWeightTable)\n",
    "        \n",
    "        # update parameters\n",
    "        self.biases -= lr * sgOfBiases.reshape(len(self.biases))\n",
    "        self.weightTable -= lr * sgOfWeightTable\n",
    "        \n",
    "        # reset rawOutputs\n",
    "        self.rawOutputs = []\n",
    "        self.outputs = []\n",
    "        \n",
    "        return gradients\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-6330de558d60>, line 164)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6330de558d60>\"\u001b[0;36m, line \u001b[0;32m164\u001b[0m\n\u001b[0;31m    return results\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "## DNN\n",
    "\n",
    "class DNN:\n",
    "    \"\"\"\n",
    "    < Deep Neuro Network >\n",
    "    \"\"\"\n",
    "    def __init__(self, learningRate, costFunction):\n",
    "        \"\"\"\n",
    "        learningRate <Float> : learning rate to do SGD.\n",
    "        costFunction <String> : lower case string. e.g. LMS, cross-entropy\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.lr = learningRate\n",
    "        self.costFunction = costFunction.lower()\n",
    "        self.regularizer = None\n",
    "    def add(self, numberOfNodes, numberOfInput = None, activeFunc = ActiveFunctions()):\n",
    "        \"\"\"\n",
    "        Add a new layer into Deep Neuro Network\n",
    "        \"\"\"\n",
    "        if not self.layers:\n",
    "            # the first layer\n",
    "            self.layers.append(Layer(numberOfInput, numberOfNodes, activeFunc))\n",
    "        else:\n",
    "            self.layers.append(Layer(self.layers[-1].numberOfNodes, numberOfNodes, activeFunc))\n",
    "            \n",
    "    def forwardPropagation(self, inputDatas, toUpdate = False):\n",
    "        propagationInputs = inputDatas\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            propagationInputs = layer.computeOutputs(propagationInputs, toUpdate)\n",
    "            \n",
    "        return propagationInputs\n",
    "    def backPropagation(self, miniBatch, toPrint = False):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "        ------------------------\n",
    "        miniBatch <tuple> (datas, targets)\n",
    "        datas <list of numpy.ndarray> : a list storing input datas\n",
    "        targets <list of numpy.ndarray> : a list storing targets correspond to datas\n",
    "        \"\"\"\n",
    "        datas, targets = miniBatch\n",
    "        # forward propagation\n",
    "        outputs = self.forwardPropagation(datas, toUpdate = True)\n",
    "        \n",
    "        preGradients = []\n",
    "        # take gradient to output\n",
    "        if self.costFunction == 'lms':\n",
    "            for t, p in zip(targets, outputs):\n",
    "                preGradients.append(-2*(t - p))\n",
    "            preGradients = np.array(preGradients)\n",
    "        \n",
    "        elif self.costFunction == 'cross-entropy':\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        else:\n",
    "            assert False, \"No Support for {} cost function\".format(self.costFunction)\n",
    "            \n",
    "        for idx in reversed(range(len(self.layers))):\n",
    "            if toPrint: print('Propagation to #{} layer'.format(idx))\n",
    "                \n",
    "            if idx > 0:\n",
    "                preGradients = self.layers[idx].update(preGradients, self.layers[idx - 1].outputs, self.lr)\n",
    "            else:\n",
    "                preGradients = self.layers[idx].update(preGradients, datas, self.lr)\n",
    "            \n",
    "            if toPrint: print('Pregradints:', preGradients)\n",
    "                \n",
    "    def fit(self, trainingSet, stopCriterion=0.001, maxEpochs=30000, miniBatchSize=64, toPlot=False, toPrint=False):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "        ---------------------\n",
    "        1. trainingSet: (trainingDatas, trainingTargets)\n",
    "            * trainingTargets: <numpy.ndarray>, shape = (#samples, dimOfTargets)\n",
    "            * trainingDatas: <numpy.ndarray>, shape=(#samples, #features) \n",
    "        2. stopCriterion: <float>\n",
    "        3. maxEpochs: <Int>\n",
    "        \n",
    "        \"\"\"\n",
    "        DATA = 0\n",
    "        TARGET = 1\n",
    "        numMiniBatches = len(trainingSet) // miniBatchSize\n",
    "        errs = []\n",
    "        err = 1.0\n",
    "        numEpochs = 0\n",
    "        \n",
    "        while (numEpochs < maxEpochs): #and (err > stopCriterion):\n",
    "            numEpochs += 1\n",
    "            if toPrint: print('Learning Epoch #{} ...'.format(numEpochs))\n",
    "            \n",
    "            # shuffle training datas for partition mini batch\n",
    "            np.random.shuffle(trainingSet)\n",
    "            \n",
    "            # Retrieve Data and Target\n",
    "            datas = [x[DATA] for x in trainingSet]\n",
    "            datas = np.array(datas)\n",
    "            targets = [x[TARGET] for x in trainingSet]\n",
    "            targets = np.array(targets)\n",
    "            \n",
    "            for i in range(numMiniBatches):\n",
    "                # Slicing\n",
    "                miniBatchDatas = datas[i*miniBatchSize:(i+1)*miniBatchSize]\n",
    "                miniBatchTargets = targets[i*miniBatchSize:(i+1)*miniBatchSize]\n",
    "                \n",
    "                self.backPropagation((miniBatchDatas, miniBatchTargets))\n",
    "            \n",
    "            if self.costFunction == 'lms':\n",
    "                predict = np.array(self.forwardPropagation(datas))\n",
    "                err = predict.reshape(targets.shape) - targets\n",
    "                err = sum(err ** 2) / float(len(targets))\n",
    "                errs.append(err)\n",
    "                err = np.linalg.norm(err) \n",
    "        \n",
    "        if toPlot:\n",
    "            plt.figure()\n",
    "            plt.plot(list(range(1, numEpochs + 1)), errs, label='Error History')\n",
    "            plt.xlabel('# Epochs')\n",
    "            plt.ylabel('RMS Error')\n",
    "            plt.title('Training History')\n",
    "      \n",
    "    def evaluate(self, testingSet):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "        ---------------------\n",
    "        1. testingSet: (trainingDatas, trainingTargets)\n",
    "            * testingTargets: <numpy.ndarray>, shape = (#samples, dimOfTargets)\n",
    "            * testingDatas: <numpy.ndarray>, shape=(#samples, #features) \n",
    "        \n",
    "        \"\"\"\n",
    "        DATA = 0\n",
    "        TARGET = 1\n",
    "        \n",
    "        datas = [x[DATA] for x in testingSet]\n",
    "        datas = np.array(datas)\n",
    "        targets = [x[TARGET] for x in testingSet]\n",
    "        targets = np.array(targets)\n",
    "        \n",
    "        results = np.array(self.forwardPropagation(datas))\n",
    "        err = results.reshape(targets.shape) - targets\n",
    "        err = sum(err ** 2) / float(len(err))\n",
    "        \n",
    "        return err\n",
    "    \n",
    "    def predict(self, testingSet, toPlot = False):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "        ---------------------\n",
    "        1. testingSet: (trainingDatas, trainingTargets)\n",
    "            * testingTargets: <numpy.ndarray>, shape = (#samples, dimOfTargets)\n",
    "            * testingDatas: <numpy.ndarray>, shape=(#samples, #features) \n",
    "        \n",
    "        \"\"\"\n",
    "        DATA = 0\n",
    "        TARGET = 1\n",
    "        \n",
    "        datas = [x[DATA] for x in testingSet]\n",
    "        datas = np.array(datas)\n",
    "        targets = [x[TARGET] for x in testingSet]\n",
    "        targets = np.array(targets)\n",
    "        \n",
    "        results = np.array(self.forwardPropagation(datas))\n",
    "        \n",
    "        if toPlot:\n",
    "            x_axis = list(range(1, len(targets) + 1))\n",
    "            plt.figure()\n",
    "            plt.plot(x_axis, targets, label = 'Target')\n",
    "            plt.plot(x_axis, results, label = 'Prediction')\n",
    "            plt.xlabel('case No.')\n",
    "            plt.ylabel('heat load')\n",
    "            plt.title('heating load prediction')\n",
    "        \n",
    "        return results\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    dnn = DNN(0.001, 'LMS')\n",
    "    dnn.add(numberOfInput = 5, numberOfNodes = 10, activeFunc = ReLu())\n",
    "    dnn.add(numberOfNodes = 3, activeFunc = ReLu())\n",
    "    dnn.add(numberOfNodes = 1, activeFunc = ActiveFunctions())\n",
    "    datas = [np.array([1, 2, 3, 4, 5]), np.array([1, 0, 0, 5, 6])]\n",
    "    #print(dnn.forwardPropagation(datas))\n",
    "    \n",
    "    targets = [1, 10]\n",
    "    dnn.fit(list(zip(datas, targets)), miniBatchSize = 1, toPlot = True)\n",
    "    \n",
    "#     for i in range(10000):\n",
    "#         dnn.backPropagation((datas, targets))\n",
    "    \n",
    "    print(dnn.forwardPropagation(datas))\n",
    "    print(dnn.predict(list(zip(datas, targets))))\n",
    "#     count = 0\n",
    "#     for layer in dnn.layers:\n",
    "#         if layer.rawOutputs != []: count += 1\n",
    "#         if layer.outputs != []: count += 1\n",
    "#     print('count:', count)\n",
    "            \n",
    "#     print(dnn.layers[0].rawOutputs)\n",
    "#     print(dnn.layers[1].outputs)\n",
    "    \n",
    "#     print((dnn.layers[0].weightTable.dot(np.array([1, 2, 3, 4, 5])) + dnn.layers[0].biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
